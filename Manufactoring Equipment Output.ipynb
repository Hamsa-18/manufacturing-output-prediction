# -*- coding: utf-8 -*-
"""Manufactoring Equipment Output.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rpkt600_cR6XHQrYlFz8iAXsS-SBJz3a

1. Setup — imports & constants
"""

# Step 1: imports and constants
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib

# File path (change if needed)
DATA_PATH = ("/content/manufacturing_dataset_1000_samples (1).csv")
RANDOM_STATE = 42

"""2. Load the data and inspect"""

# Step 2: load and inspect
df = pd.read_csv("/content/manufacturing_dataset_1000_samples (1).csv")
print("Rows, Columns:", df.shape)
df.head(5)

"""3. Check for missing values and column names"""

# Step 3: missing values and columns
print("Columns:", df.columns.tolist())
print("\nMissing values:\n", df.isnull().sum())

"""4. Simple EDA: distribution of target and a pairwise view"""

# Step 4: simple exploratory plots
plt.figure(figsize=(7,4))
sns.histplot(df['Parts_Per_Hour'], bins=30, kde=True)
plt.title('Parts_Per_Hour distribution')
plt.show()

# Quick scatter: Parts_Per_Hour vs Cycle_Time (example)
plt.figure(figsize=(6,4))
plt.scatter(df['Cycle_Time'], df['Parts_Per_Hour'], alpha=0.6)
plt.xlabel('Cycle_Time')
plt.ylabel('Parts_Per_Hour')
plt.title('Parts_Per_Hour vs Cycle_Time')
plt.show()

"""5. Handle missing values (simple and transparent)"""

# Step 5: handle missing values in an explicit, simple way

# columns mentioned earlier as having missing values
# we'll fill numeric columns with median (robust) and categorical with mode

numeric_fill_cols = ['Material_Viscosity', 'Ambient_Temperature', 'Operator_Experience']
for c in numeric_fill_cols:
    if c in df.columns:
        median_val = df[c].median()
        df[c] = df[c].fillna(median_val)
        print(f"Filled {c} missing with median: {median_val}")

# If any categorical had missing, fill with most frequent value (example)
cat_cols = ['Shift', 'Machine_Type', 'Material_Grade', 'Day_of_Week']
for c in cat_cols:
    if c in df.columns:
        mode_val = df[c].mode()[0]
        df[c] = df[c].fillna(mode_val)
        print(f"Filled {c} missing with mode: {mode_val}")

"""6. Prepare features and target

"""

# Step 6: features and target
target = 'Parts_Per_Hour'

# define categorical and numeric feature lists (explicit so teacher can see)
categorical_features = ['Shift', 'Machine_Type', 'Material_Grade', 'Day_of_Week']
# numeric: take all numeric columns except the target
numeric_features = [c for c in df.select_dtypes(include=np.number).columns.tolist() if c != target]

print("Numeric features:", numeric_features)
print("Categorical features:", categorical_features)

# X and y
X = df[numeric_features + [c for c in categorical_features if c in df.columns]]
y = df[target]

"""7. Split into train and test"""

# Step 7: train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE
)
print("Train size:", X_train.shape, "Test size:", X_test.shape)

"""8. Preprocessing pipelines (impute again + scale + encode)"""

# Step 8: preprocessor: simple and clear

# numeric transformer: impute (median) then scale
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# categorical transformer: impute (most frequent) then one-hot encode
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, [c for c in categorical_features if c in X.columns])
])

"""9. Train a simple Linear Regression model (baseline)"""

# Step 9: pipeline with linear regression
model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])

# train
model_pipeline.fit(X_train, y_train)
print("Model trained (Linear Regression).")

"""10. Evaluate the model (RMSE, MAE, R2)"""

# Step 10: evaluate on test set
y_pred = model_pipeline.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Test MSE: {mse:.3f}")
print(f"Test RMSE: {rmse:.3f}")
print(f"Test MAE: {mae:.3f}")
print(f"Test R2: {r2:.3f}")

# simple plot: true vs predicted
plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('True Parts_Per_Hour')
plt.ylabel('Predicted Parts_Per_Hour')
plt.title('True vs Predicted (Test set)')
plt.show()

"""11. (Optional) Try a slightly better regularized model (Ridge)"""

# Step 11: try Ridge regression for small improvement
ridge_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', Ridge(alpha=1.0, random_state=RANDOM_STATE))
])

ridge_pipeline.fit(X_train, y_train)
y_pred_ridge = ridge_pipeline.predict(X_test)

rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))
print(f"Ridge RMSE on test: {rmse_ridge:.3f}")

# Choose better of the two
if rmse_ridge < rmse:
    final_model = ridge_pipeline
    print("Ridge selected as final model.")
else:
    final_model = model_pipeline
    print("Linear Regression kept as final model.")

"""12. Save the final model to disk"""

# Step 12: save model to model/model.joblib
os.makedirs("model", exist_ok=True)
joblib.dump(final_model, "model/model.joblib")
print("Saved final model to model/model.joblib")

# Step 14: map coefficients to feature names (only for linear models)
# this works if linear or ridge is used; get preprocessor and regressor
pre = final_model.named_steps['preprocessor']
reg = final_model.named_steps['regressor']

# numeric feature names
num_names = numeric_features

# categorical feature names after one-hot
ohe = pre.named_transformers_['cat'].named_steps['onehot']
cat_names = list(ohe.get_feature_names_out([c for c in categorical_features if c in X.columns]))

feature_names = num_names + cat_names
coefficients = reg.coef_

coef_df = pd.DataFrame({'feature': feature_names, 'coefficient': coefficients})
coef_df = coef_df.reindex(coef_df.coefficient.abs().sort_values(ascending=False).index)
coef_df.head(10)

import pickle
import json
import joblib

# model saving
with open('linear_regression_model.pkl','wb') as f:
  pickle.dump(final_model, f)
feature_columns = X.columns.tolist()
with open("feature_columns.json", "w") as f:
  json.dump(feature_columns, f)


print("model, scaler (if part of pipeline), and feature columns saved.")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Separate features and target
X = df.drop(columns=['Parts_Per_Hour'])
y = df['Parts_Per_Hour']

# 1️⃣ One-hot encode categorical columns
X = pd.get_dummies(X, drop_first=True)

# 2️⃣ Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3️⃣ Scale numeric features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4️⃣ Train your model
from sklearn.linear_model import LinearRegression
final_model = LinearRegression()
final_model.fit(X_train_scaled, y_train)

# 5️⃣ Save both model and scaler
import joblib
joblib.dump(final_model, 'linear_regression_model.pkl')
joblib.dump(scaler, 'scaler.pkl')

print("✅ Model and scaler have been saved successfully!")

!ls

import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
import pandas as pd
import numpy as np

# Assuming 'df' is already loaded from a previous cell
# Step 2: load and inspect
DATA_PATH = ("/content/manufacturing_dataset_1000_samples (1).csv")
df = pd.read_csv(DATA_PATH)

# Step 5: handle missing values
numeric_fill_cols = ['Material_Viscosity', 'Ambient_Temperature', 'Operator_Experience']
for c in numeric_fill_cols:
    if c in df.columns:
        median_val = df[c].median()
        df[c] = df[c].fillna(median_val)

# Separate features and target
X = df.drop(columns=['Parts_Per_Hour'])
y = df['Parts_Per_Hour']

# Define numeric and categorical features
numeric_features = [c for c in X.select_dtypes(include=np.number).columns.tolist()]
categorical_features = [c for c in X.select_dtypes(include='object').columns.tolist()]

# Create preprocessing pipelines for numeric and categorical features
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Create a column transformer to apply different transformations to different columns
preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features),
    ('cat', categorical_transformer, categorical_features)
])


# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Train your model
final_model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])

final_model.fit(X_train, y_train)

# Save the trained model and scaler
joblib.dump(final_model, 'linear_regression_model.pkl')
# The scaler is now part of the pipeline, so we don't need to save it separately.
# joblib.dump(scaler, 'scaler.pkl')


print("✅ Model and scaler saved successfully!")
